{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import EventCollection\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.bipartite.generators import complete_bipartite_graph\n",
    "from numpy import linalg as LA\n",
    "import csv\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calling Condat's Fast projection to L1 ball \n",
    "Reference: Condat L., Fast projection onto the simplex and the ℓ1 ball. Mathematical\n",
    "Programming, 2016\n",
    "\"\"\"\n",
    "\n",
    "%run ./Fast_projection_simplex_l1ball.ipynb\n",
    "\n",
    "\"\"\"\n",
    "Calling Dykstra algorithm to project on the intersection of L1 and L2 ball\n",
    "Reference: Birgin, E.G., Raydan, M.: Robust stopping criteria for dykstra’s algorithm.\n",
    "SIAM Journal on Scientific Computing, 2005\n",
    "\"\"\"\n",
    "\n",
    "%run ./Dykstra_projection_L1_ball.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download data set a4a from LIBSVM\n",
    "Website: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\n",
    "Load csv file of downloaded data \n",
    "\"\"\"\n",
    "\n",
    "mydata = np.genfromtxt('a4a3.csv',delimiter = ',')\n",
    "num_features = len(mydata[0]) - 1\n",
    "num_samples = len(mydata)\n",
    "print('num samples: %d num features : %d ' %(num_samples, num_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shuffling the indices\n",
    "\"\"\"\n",
    "\n",
    "indices = np.arange(len(mydata))\n",
    "np.random.seed(100)\n",
    "np.random.shuffle(indices)\n",
    "print ('indices after shuffling',indices)\n",
    "shuffled_data = [list(mydata[i]) for i in indices]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Separating features and labels from shuffled data\n",
    "\"\"\"\n",
    "\n",
    "shuffled_data = np.array(shuffled_data)\n",
    "X1 = shuffled_data[:,1:(num_features + 1)]\n",
    "Y1 = shuffled_data[:,:1]\n",
    "print (len(X1[0]))\n",
    "X = [X1[i]/LA.norm(X1[i]) for i in range(len(X1))] ## Normalize feature vectors\n",
    "Y = [Y1[i][0] for i in range(len(Y1)) ]\n",
    "\n",
    "\"\"\"\n",
    "Adding 1 to each feature vector to incorporate offset parameter\n",
    "\"\"\"\n",
    "\n",
    "X = [ np.insert(X[i],0,1) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_blocks(X,Y,m):\n",
    "    \"\"\"\n",
    "    This function distributes features and corresponding lables to m nodes\n",
    "\n",
    "    Input\n",
    "    ---------\n",
    "     X: feature matrix, Y: labels, m: number of nodes\"\n",
    "\n",
    "     Returns\n",
    "     --------\n",
    "    features: A list containing features of all nodes\n",
    "    values: A list containing associated labels of all nodes\n",
    "\n",
    "    \"\"\"\n",
    "    BX = [[] for i in range(m)]\n",
    "    BY = [[] for i in range(m)]\n",
    "    d = len(X)\n",
    "    s2 = int(d/m)  ## size of each block\n",
    "    g = 0\n",
    "    for i in range(m):\n",
    "        for j in range(g,s2 + g,1):\n",
    "            BX[i].append(X[j])\n",
    "            BY[i].append(Y[j])\n",
    "        g = g + s2 \n",
    "    samples_after_eq_dist = m*int(d/m)  ## total samples after equally distributed samples to every node\n",
    "    remaining_samples = d - m*int(d/m) ### This quantity will always be less than equal to m\n",
    "    if ( remaining_samples >= 1):\n",
    "        for j in range(remaining_samples):\n",
    "            BX[j].append(X[samples_after_eq_dist + j])\n",
    "            BY[j].append(Y[samples_after_eq_dist + j])\n",
    "\n",
    "    return [BX,BY]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 2D Torus topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_graph(n=0,create_using=None):\n",
    "    \"\"\"Return the empty graph with n nodes and zero edges.\n",
    "\n",
    "    Node labels are the integers 0 to n-1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if create_using is None:\n",
    "        # default empty graph is a simple graph\n",
    "        G=nx.Graph()\n",
    "    else:\n",
    "        G=create_using\n",
    "        G.clear()\n",
    "\n",
    "    G.add_nodes_from(range(n))\n",
    "    G.name=\"empty_graph(%d)\"%n\n",
    "    return G\n",
    "\n",
    "\n",
    "def grid_2d(m,n,periodic=False,create_using=None): ## m,n be the number of rows and number of \n",
    "    # columns in torus topolgy\n",
    "    \n",
    "    \"\"\" Return the 2d grid graph of mxn nodes,\n",
    "        each connected to its nearest neighbors.\n",
    "        Optional argument periodic=True will connect\n",
    "        boundary nodes via periodic boundary conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    G=empty_graph(0,create_using)\n",
    "    G.name=\"grid_2d_graph\"\n",
    "    rows=range(m)\n",
    "    columns=range(n)\n",
    "    G.add_nodes_from( (i,j) for i in rows for j in columns )\n",
    "    G.add_edges_from( ((i,j),(i-1,j)) for i in rows for j in columns if i>0 )\n",
    "    G.add_edges_from( ((i,j),(i,j-1)) for i in rows for j in columns if j>0 )\n",
    "    if G.is_directed():\n",
    "        G.add_edges_from( ((i,j),(i+1,j)) for i in rows for j in columns if i<m-1 )\n",
    "        G.add_edges_from( ((i,j),(i,j+1)) for i in rows for j in columns if j<n-1 )\n",
    "    if periodic:\n",
    "        if n>2:\n",
    "            G.add_edges_from( ((i,0),(i,n-1)) for i in rows )\n",
    "            if G.is_directed():\n",
    "                G.add_edges_from( ((i,n-1),(i,0)) for i in rows )\n",
    "        if m>2:\n",
    "            G.add_edges_from( ((0,j),(m-1,j)) for j in columns )\n",
    "            if G.is_directed():\n",
    "                G.add_edges_from( ((m-1,j),(0,j)) for j in columns )\n",
    "        G.name=\"periodic_grid_2d_graph(%d,%d)\"%(m,n)\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing weight matrix W associated with 2D-Torus topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_graph(row,column,m,w1,w2):\n",
    "    \n",
    "    \"\"\"\n",
    "    row = column + 1\n",
    "    m: number of nodes\n",
    "    w1, w2: weights to generate entries of W \n",
    "    \"\"\"\n",
    "    \n",
    "    A = [[0 for j in range(m)] for i in range(m)] # weight matrix\n",
    "    W = [[0 for j in range(m)] for i in range(m)] # weight matrix\n",
    "    \n",
    "    \"Generating 2D Grid\"\n",
    "    G = grid_2d(row,column,periodic=False,create_using=None)\n",
    "    \n",
    "#     \"Adding extra edges to get 2D Torus\"\n",
    "#     edges_rows = [((0,0),(0,4)),((1,0),(1,4)),((2,0),(2,4)),((3,0),(3,4))] \n",
    "#     column_rows = [((0,0),(3,0)),((0,1),(3,1)),((0,2),(3,2)),((0,3),(3,3)),((0,4),(3,4))] \n",
    "#     G.add_edges_from(edges_rows)\n",
    "#     G.add_edges_from(column_rows)\n",
    "    nx.draw_networkx(G)\n",
    "    plt.savefig('2D-Grid')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \"Changing edges format to 1D so that it becomes easy to access the indices\"\n",
    "    \n",
    "    edges = []\n",
    "    for i in range(row):\n",
    "        for j in range(column-1):\n",
    "            edges.append([j+i*column,j+1+i*column])\n",
    "\n",
    "    for i in range(row-1):\n",
    "        for j in range(column):\n",
    "            edges.append([j+i*column,j+(i+1)*column])      \n",
    "\n",
    "    \"Adding extra edges to get 2D Torus from 2D grid\"\n",
    "\n",
    "    for i in range(column):\n",
    "        edges.append([i , i + (row-1)*column])\n",
    "\n",
    "    for i in range(row):\n",
    "        edges.append([i*column , row + i*column]) \n",
    "\n",
    "    print ('edges:',edges)   \n",
    "    print ('total edges in 2D Torus:',len(edges))\n",
    "    \n",
    "    for [u, v] in edges:\n",
    "        W[u][v] = rd.uniform(w1,w2)\n",
    "        W[v][u] = rd.uniform(w1,w2)\n",
    "\n",
    "    for i in range(m):\n",
    "        W[i][i] = w1+3*w2\n",
    "       \n",
    "    for i in range(m):\n",
    "        s1 = sum(W[i])\n",
    "        W[i] = [W[i][j]/s1 for j in range(m)]       \n",
    "    for [u, v] in edges:\n",
    "        A[u][v] = min(W[u][v], W[v][u])\n",
    "        A[v][u] = min(W[u][v], W[v][u])\n",
    "    \n",
    "    for i in range(m):\n",
    "        A[i][i] = 1 - sum(A[i])\n",
    "\n",
    "    \n",
    "    B = np.matrix(A)\n",
    "    print ((B.transpose() == A).all()) ## check symmetric property of A. It will print True\n",
    "    print ([sum(A[i]) for i in range(m)])\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiConsensus(W,m,x,tau):\n",
    "    \n",
    "    \"\"\"\n",
    "    W: weight matrix obtained from connected graph\n",
    "    tau: number of communication rounds\n",
    "    Output: This function returns approximation of the average of x_1, x_2, ...,x_m\n",
    "    where x = [x_1,...,x_m]\n",
    "    \"\"\"\n",
    "    \n",
    "    v = [[0 for i in range(len(x[0]))] for j in range(m)]\n",
    "    x_new = np.copy(x)\n",
    "    for t in range(int(tau)):\n",
    "        x_old = np.copy(x_new)\n",
    "        for i in range(m):\n",
    "            v2 = [W[i][j]*np.array(x_old[j]) for j in range(m)]\n",
    "            v2 = np.array(v2)\n",
    "            v[i] = v2.sum(axis =0)\n",
    "        x_new = np.copy(v)     \n",
    "    return x_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneConsensus(W2,m,x):\n",
    "    \"\"\"\n",
    "    This function returns the weighted sum of x_1, x_2, ...,x_m where x = [x_1,...,x_m]\n",
    "    \"\"\"\n",
    "    v = [[0 for i in range(len(x[0]))] for j in range(m)]\n",
    "    for i in range(m):\n",
    "        u = [W2[i][j]*np.array(x[j]) for j in range(m)]\n",
    "        u = np.array(u)\n",
    "        v[i] = u.sum(axis = 0)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Momentum coefficient (t-1)/(t+alpha-1)\n",
    "\"\"\"\n",
    "\n",
    "def update_t(k,alpha):\n",
    "    a1 = k\n",
    "    a2 = k + alpha \n",
    "    return a1/a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient of logistic loss function at point x\n",
    "\"\"\"\n",
    "\n",
    "def logistic_grad(bFeature,bLabel,x,lambdaa,d): ## d is the total number of samples\n",
    "    summ = np.zeros(len(bFeature[0]))\n",
    "    cof = (lambdaa)/m\n",
    "    a = len(bFeature) ## batch size\n",
    "    for i in range(a):\n",
    "        inp1 = np.dot(x,bFeature[i])\n",
    "        n1 = (-bLabel[i])*np.array(bFeature[i])\n",
    "        dom = bLabel[i]*(inp1)\n",
    "        if (dom > 0): ## To avoid math flow error\n",
    "            num1 = 1 - 1/(1 + math.exp((-1)*dom))\n",
    "            #print (num1)\n",
    "        else:\n",
    "            num1 = 1/(1 + math.exp(dom))\n",
    "        num2 = num1*np.array(n1) \n",
    "        summ = np.add(summ,num2)\n",
    "    loss_der = (1/d)*np.array(summ)\n",
    "    reg_der = cof*np.array(x)\n",
    "    gradient = np.add(loss_der,reg_der)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding logistic regression function value at point x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(X,Y,x,lambdaa): \n",
    "    \"\"\"\n",
    "    logistic function value computed at (x,y)\n",
    "    Input:\n",
    "    Features: X\n",
    "    Labels: Y\n",
    "    regcoef: lambda\n",
    "    x: Point at which function value is computed\n",
    "\n",
    "    Returns:\n",
    "    Logistic regression function value at x with dataset (X,Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    f = 0\n",
    "    d = len(X)\n",
    "    cof = (lambdaa)/2\n",
    "    xnorm = (LA.norm(x))**2\n",
    "    for i in range(d):\n",
    "        inp = np.dot(x,X[i])\n",
    "        dom = Y[i]*inp\n",
    "        if (dom > 0):\n",
    "            f1 = 1+ math.exp((-1)*dom)\n",
    "            z11 = math.log(f1)\n",
    "            f = f + z11   \n",
    "            #print (z11)\n",
    "        else:\n",
    "            f1 = 1+ math.exp(dom)\n",
    "            z11 = (-1)*dom + math.log(f1)\n",
    "            f = f + z11   \n",
    "         \n",
    "    func = (f/d) + cof*xnorm\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This function runs projected distributed accelerated gradient method (one step communication) starting from x0. \n",
    "It returns the last local iterates , r is the diameter of the orginal constraint set X, R = R_k \n",
    "\"\"\"\n",
    "\n",
    "def DAGOSC(x0,W,m,InIt,dataDis,step,r,R,epsilon_prevIte,tolerance,d): \n",
    "    \n",
    "    \"\"\"\n",
    "    x0: initial iterates x_{k,0}\n",
    "    W: weight matrix\n",
    "    m: number of nodes\n",
    "    InIt: number of inner iterates t_k\n",
    "    dataDis: local data sets\n",
    "    step: step size s_k\n",
    "    r: diameter of constraint set X\n",
    "    epsilon_prevIte: consensus error ub epsilon_k\n",
    "    \"\"\"\n",
    "    \n",
    "    x = [[0 for i in range(len(X[0]))]  for j in range(m)] ## store iterates x_k of all agents at current iterate\n",
    "    x_p = [[0 for i in range(len(X[0]))]  for j in range(m)] ## projected iterate\n",
    "    func = np.zeros(InIt) # function values\n",
    "    z1 = x0.copy()\n",
    "    y = x0.copy()\n",
    "    for t in range(InIt):\n",
    "        v3 = oneConsensus(W,m,y)\n",
    "        for i in range(m):\n",
    "            feature = dataDis[0][i] ## ith block of data\n",
    "            label = dataDis[1][i]\n",
    "            grad = logistic_grad(feature,label,y[i],lambdaa,d)\n",
    "            step_grad = step*np.array(grad)\n",
    "            x[i] = np.subtract(v3[i],step_grad)\n",
    "            \"\"\"\n",
    "            project x_i onto the intersection of X and X^i_k\n",
    "            \"\"\"\n",
    "#             print ('x[0]',x[0][0])\n",
    "            radius_new = R + epsilon_prevIte ### R_k + epsilon(k)\n",
    "            x_p[i] = Dikstra(x[i],x0[i],r,radius_new,tolerence)\n",
    "#             print ('x_p[0]',x_p[0][0])\n",
    "#         print ('inner iteration',t+1)\n",
    "        if (t == 0):\n",
    "            y = x_p.copy()\n",
    "        else:\n",
    "            mom1 = np.subtract(x_p,z1)\n",
    "            beta = update_t(t,alpha)\n",
    "            mom2 = beta*np.array(mom1)\n",
    "            y = np.add(x_p,mom2)\n",
    "        z1 = x_p.copy() ## store x^i of previous iterate\n",
    "        avg_estimates = np.mean(x_p,axis = 0)\n",
    "        func[t] = logistic_func(X,Y,avg_estimates,lambdaa)\n",
    "        function_mpdaca.write(str(func[t])+'\\n')\n",
    "        function_mpdaca.flush()    \n",
    "    return x_p  ## projected local iterates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" inner iterations \"\n",
    "def innIt(c,m,L,mu,B,varepsilon_0,k):\n",
    "    s0 = (m*L*(1-mu)*varepsilon_0)/(16*B**2)\n",
    "    t1 = (4*(alpha -1)*math.sqrt(m))/(math.sqrt(L*s0))\n",
    "    t2 = 1.5*(1+L*c**2) + 1/16\n",
    "#     print (t1,t2,t3,t4)\n",
    "    return t1*t2*(math.sqrt(2)**k)\n",
    "\n",
    "\"\"\"\n",
    "updating varepsilon, epsilon, epsilon_tilde\n",
    "\"\"\"\n",
    "\n",
    "def varepsilon_k(varepsilon_prevIte):\n",
    "    varepsilon = varepsilon_prevIte/2\n",
    "    epsilon = (math.sqrt(varepsilon))/16\n",
    "    epsilon_tilde = (math.sqrt(varepsilon))/8\n",
    "    return varepsilon, epsilon, epsilon_tilde\n",
    "\n",
    "def tau_iterates(epsilon,m,mu,radius):\n",
    "    den = (-1)*(math.log(mu))\n",
    "    num = math.log(math.sqrt(2*m)*(radius+epsilon)) - math.log(epsilon)\n",
    "    tau = num/den\n",
    "    return math.ceil(tau)\n",
    "\n",
    "def tau_tilde(epsilon_tilde,epsilon,mu,B,radius):\n",
    "    C1 = m*epsilon + B/L\n",
    "    den = (-1)*(math.log(mu))\n",
    "    num = math.log(C1) - math.log(epsilon_tilde)\n",
    "    tau_tilde = num/den\n",
    "    return math.ceil(tau_tilde)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mPDACA(X,Y,T,m,W,d,alpha,zeta,c_0,radius):\n",
    "    norm_proximal_grad = np.zeros(T+1) ## stores maximum local proximal gradient square norm\n",
    "    z = [[0 for i in range(len(X[0]))]  for j in range(m)] \n",
    "    func = logistic_func(X,Y,z[0],lambdaa)\n",
    "    function_mpdaca.write(str(func)+'\\n')\n",
    "    function_mpdaca.flush()\n",
    "    tauIt = np.zeros(T) ## tau = stores number of communications in consensus on iterates\n",
    "    tauGrad_step = np.zeros(T) ## tau_tilde = stores number of communications in consensus on gradient descent step\n",
    "    c_val = np.zeros(T) ## stores c_k at every outer iterate\n",
    "    innIter = np.zeros(T)\n",
    "    cycles = np.ones(T) ## stores number of cycles at every outer iterate k\n",
    "    xCons = np.copy(z)\n",
    "    xCons_old = np.copy(z) ## consensus iterates at previous outer iterate\n",
    "    \n",
    "    \"Initialize varpsilon_0\"\n",
    "    dataDis = data_blocks(X,Y,m)\n",
    "    sum_norm_grad_f_i = 0 ### sum of square norm of  \\nabla f_i(x^i_0)\n",
    "    grad_f_i = [] ### store local gradients at initial points x^i_0\n",
    "    for i in range(m):\n",
    "        bFeature = dataDis[0][i] ## ith block of data\n",
    "        bLabel = dataDis[1][i]\n",
    "        grad = logistic_grad(bFeature,bLabel,z[0],lambdaa,d)\n",
    "        step_grad = (-1/L)*(np.array(grad))\n",
    "        grad_f_i.append(step_grad)\n",
    "        grad_norm = (LA.norm(grad))**2\n",
    "        sum_norm_grad_f_i = sum_norm_grad_f_i + grad_norm\n",
    "    norm_v1_0 = (1/L)*math.sqrt(sum_norm_grad_f_i) ### since x_0 = z = 0\n",
    "    \"tilde{tau}_1\"\n",
    "    numerator = math.log(norm_v1_0*(10**10)) ### take epsilon_tilde(1) = 10^-10\n",
    "    denominator = (-1)*math.log(mu)\n",
    "    tau_tilde0 = numerator/denominator\n",
    "    v1_tilde = multiConsensus(W,m,grad_f_i,int(tau_tilde0))  ### \\tilde{v_1,0}\n",
    "    project_v1_tilde = projection_l1_ball(v1_tilde[0], radius)  ## choose i0 = 0 (node 1) \n",
    "    q_1_square_norm = LA.norm(project_v1_tilde)**2\n",
    "    norm_proximal_grad[0] = q_1_square_norm\n",
    "    proximal_grad_norm.write(str(norm_proximal_grad[0])+'\\n')\n",
    "    proximal_grad_norm.flush()\n",
    "    varepsilon_0 = 0.1\n",
    "    varepsilon_prevIte = varepsilon_0/2 ## varepsilon_1\n",
    "    \n",
    "    \"Initialize s^'_0\"\n",
    "    \n",
    "    step = (m*varepsilon_0*L*(1-mu))/(16*(B**2))\n",
    "#     step = (s0*accuracy)/varepsilon_0\n",
    "    \"Initialize R_1 and epsilon_1\"\n",
    "    \n",
    "    epsilon_prevIte = 0 ## because all x^i's are intialized to same vector 0 and hence epsilon(1) = 0\n",
    "    epsilon_tilde_prevIte = 10**(-10) ## epsilon_tilde(1) = = 10^-10 \n",
    "    c = c_0\n",
    "#     R = 2*(1+(c**2)*m*L)*(math.sqrt(varepsilon_prevIte) + 2*epsilon + epsilon_tilde)\n",
    "    \n",
    "    \"\"\"\n",
    "    Outer iteration\n",
    "    \"\"\"\n",
    "    for k in range(T):\n",
    "        step = step/2\n",
    "        print ('step size at k =',k+1,'is',step)\n",
    "\n",
    "        InIt = innIt(c,m,L,mu,B,varepsilon_0,k+1)\n",
    "        InIt = int(InIt)\n",
    "        innIter[k] = InIt\n",
    "        print ('inner iterations at k =',k+1,'are',InIt)\n",
    "\n",
    "#         InIt = 10**4\n",
    "        inner_iterates_cycles.write(str(innIter[k])+'\\n')\n",
    "        inner_iterates_cycles.flush()\n",
    "        R = 2*(1+(c**2)*L)*(math.sqrt(varepsilon_prevIte) + 2*epsilon_prevIte + epsilon_tilde_prevIte)\n",
    "        print ('R:',R)\n",
    "        print ('varepsilon:',varepsilon_prevIte)\n",
    "        z = DAGOSC(xCons_old,W,m,InIt,dataDis,step,radius,R,epsilon_prevIte,tolerence,d)\n",
    "        varepsilon, epsilon, epsilon_tilde = varepsilon_k(varepsilon_prevIte)\n",
    "#         print ('varepsilon_prevIte after computing varepsilon',varepsilon_prevIte)\n",
    "\n",
    "        tauIt[k] = tau_iterates(epsilon,m,mu,radius)\n",
    "        num_comm_iterates.write(str(tauIt[k])+'\\n')\n",
    "        num_comm_iterates.flush()\n",
    "        xCons = multiConsensus(W,m,z,tauIt[k])\n",
    "        v_k = [[0 for i in range(len(X[0]))]  for j in range(m)]\n",
    "        for i in range(m):\n",
    "            feature = dataDis[0][i] ## ith block of data\n",
    "            label = dataDis[1][i]\n",
    "            local_grad = logistic_grad(feature,label,xCons[i],lambdaa,d)\n",
    "            step_localGrad = (1/L)*np.array(local_grad)\n",
    "            v_k[i] = np.subtract(xCons[i],step_localGrad)\n",
    "        \n",
    "        \"Consensus on v_k+1 to compute v_tilde_k+1\"\n",
    "        \n",
    "        tauGrad_step[k] = tau_tilde(epsilon_tilde,epsilon,mu,B,radius)        ## tilde{tau}_k\n",
    "        num_comm_gradients.write(str(tauGrad_step[k])+'\\n')\n",
    "        num_comm_gradients.flush()\n",
    "        v_tilde = multiConsensus(W,m,v_k,tauGrad_step[k])\n",
    "        \n",
    "        proximal_grad = [] ## stores q_i at consensus iterate xCons^i\n",
    "        for i in range(m):\n",
    "            proj = projection_l1_ball(v_tilde[i], radius)\n",
    "            diff = np.subtract(xCons[i],proj)\n",
    "            square_norm = (LA.norm(diff))**2\n",
    "            proximal_grad.append(square_norm)\n",
    "        \n",
    "        check = [proximal_grad[i] <= varepsilon for i in range(m) ]\n",
    "        while (any(check) == False): ## any(check) returns True if atleast one is True in check array otherwie returns False\n",
    "            cycles[k] += 1\n",
    "            np.savetxt('cycles.txt',cycles)\n",
    "            c = zeta*c\n",
    "            print ('c update',c)\n",
    "            InIt = innIt(c,m,L,mu,B,varepsilon_0,k+1,accuracy)\n",
    "            innIter[k] = InIt\n",
    "            print ('inner iterations at k =',k+1,'are',InIt)\n",
    "\n",
    "#             InIt = 10**4\n",
    "\n",
    "            inner_iterates_cycles.write(str(innIter[k])+'\\n')\n",
    "            inner_iterates_cycles.flush()\n",
    "            R = 2*(1+(c**2)*L)*(math.sqrt(varepsilon_prevIte) + 2*epsilon_prevIte + epsilon_tilde_prevIte)\n",
    "            print ('R:',R)\n",
    "            z = DAGOSC(xCons_old,W,m,InIt,dataDis,step,radius,R,epsilon_prevIte,tolerence,d)\n",
    "#             tauIt[k] = tau_iterates(epsilon,m,r,mu)\n",
    "#             num_comm_iterates.write(str(tauIt[k])+'\\n')\n",
    "#             num_comm_iterates.flush()\n",
    "            xCons = multiConsensus(W,m,z,tauIt[k])\n",
    "            v_k = [[0 for i in range(len(X[0]))]  for j in range(m)]\n",
    "            for i in range(m):\n",
    "                feature = dataDis[0][i] ## ith block of data\n",
    "                label = dataDis[1][i]\n",
    "                local_grad = logistic_grad(feature,label,xCons[i],lambdaa,d)\n",
    "                step_localGrad = (1/L)*np.array(local_grad)\n",
    "                v_k[i] = np.subtract(xCons[i],step_localGrad)\n",
    "\n",
    "            \"Consensus on v_k+1 to compute v_tilde\"\n",
    "\n",
    "#             tauGrad_step[k] = tau_tilde(epsilon_tilde,mu,B)        ## tilde{tau}_k\n",
    "#             num_comm_gradients.write(str(tauGrad_step[k])+'\\n')\n",
    "#             num_comm_gradients.flush()\n",
    "            v_tilde = multiConsensus(W,m,v_k,tauGrad_step[k])\n",
    "            proximal_grad = [] ## stores q_i at consensus iterate xCons^i\n",
    "            for i in range(m):\n",
    "                proj = projection_l1_ball(v_tilde[i], radius)\n",
    "                diff = np.subtract(xCons[i],proj)\n",
    "                square_norm = (LA.norm(diff))**2\n",
    "                proximal_grad.append(square_norm)\n",
    "\n",
    "            check = [proximal_grad[i] <= varepsilon for i in range(m) ]\n",
    "            \n",
    "        \"store minimum local proximal gradient squre norm\"    \n",
    "        norm_proximal_grad[k+1] = min(proximal_grad)\n",
    "        proximal_grad_norm.write(str(norm_proximal_grad[k+1])+'\\n')\n",
    "        proximal_grad_norm.flush()\n",
    "        xCons_old = np.copy(xCons)  \n",
    "        c_val[k] = c\n",
    "        c_values_OuterIte.write(str(c_val[k])+'\\n')\n",
    "        c_values_OuterIte.flush()\n",
    "        varepsilon_prevIte = varepsilon\n",
    "        epsilon_prevIte = epsilon\n",
    "        epsilon_tilde_prevIte = epsilon_tilde \n",
    "        print ('outer iteration' ,k+1,'done')\n",
    "                            \n",
    "                 \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"creating files to store data\"\n",
    "\n",
    "function_mpdaca = open(r\"func_mPDACA_a4a3_2D_torus.txt\",\"w\")\n",
    "proximal_grad_norm = open(r\"minimum_local_proximal_grad_square_norm.txt\",\"w\")\n",
    "inner_iterates_cycles = open(r\"inner_iterates_involving_in_cycles.txt\",\"w\")\n",
    "num_comm_iterates = open(r\"num_comm_iterates.txt\",\"w\")\n",
    "num_comm_gradients = open(r\"num_comm_gradients.txt\",\"w\")\n",
    "c_values_OuterIte = open(r\"c_values.txt\",\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finding Lipschitz parameter L\n",
    "\"\"\"\n",
    "X3 = np.matrix(X)\n",
    "\n",
    "M1 = np.matmul(np.transpose(X3),X3)\n",
    "M = M1/len(X3)\n",
    "from numpy.linalg import eig\n",
    "values , vectors = eig(M)\n",
    "\n",
    "L = float(max(values)) + lambdaa ## L is less than equal to (maximum eigen value of (1/d)X^TX) + lambd\n",
    "print ('L:',L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"Initializations\"\n",
    "\n",
    "tolerence = 10**(-20) ## tolerance on Dikstra algorithm\n",
    "row = 4 ### total number of rows in 2D Torus\n",
    "column = 5 ### total number of columns in 2D Torus\n",
    "m = row*column ### total number of nodes\n",
    "rd.seed(2)\n",
    "W = gen_graph(row,column,m,0.04,0.05)\n",
    "from numpy.linalg import eig\n",
    "values , vectors = eig(W)\n",
    "values = np.sort(values)\n",
    "print('eigenvalues of W are',values)\n",
    "mu = values[m-2] ## second largest eigen value of W\n",
    "print ('\\mu(W) = ',mu)\n",
    "d = len(X) ## total number of samples \n",
    "T = 10**5 ## outer iterations\n",
    "alpha = 3\n",
    "c_0 = 0.1\n",
    "radius = 1 ## X is l1 unit ball\n",
    "zeta = math.sqrt(2)\n",
    "lambdaa = 0.01 ## regularization parameter\n",
    "\n",
    "## data distribution\n",
    "[BX,BY] = data_blocks(X,Y,m)\n",
    "\n",
    "\n",
    "\"maximum samples a node has after distribution\"\n",
    "\n",
    "node_maximum_samples = max([len(BY[i]) for i in range(m)])\n",
    "\n",
    "num_features = len(X[0])\n",
    "B = (node_maximum_samples/d) + (lambdaa/m)*radius ## gradient bound or Lipschitz parameter of each f_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mPDACA(X,Y,T,m,W,d,alpha,zeta,c_0,radius)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
