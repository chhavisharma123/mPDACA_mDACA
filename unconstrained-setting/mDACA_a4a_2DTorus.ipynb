{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import EventCollection\n",
    "import numpy as np\n",
    "import copy\n",
    "import random as rd\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.bipartite.generators import complete_bipartite_graph\n",
    "from numpy import linalg as LA\n",
    "import csv\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download data set a4a from LIBSVM\n",
    "Website: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\n",
    "Load csv file of downloaded data\n",
    "\"\"\"\n",
    "\n",
    "mydata = np.genfromtxt('a4a.csv',delimiter = ',')\n",
    "num_features = len(mydata[0]) - 1\n",
    "num_samples = len(mydata)\n",
    "print('num samples: %d num features : %d ' %(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shuffling the indices\n",
    "\"\"\"\n",
    "\n",
    "indices = np.arange(len(mydata))\n",
    "# print (indices)\n",
    "np.random.seed(100)\n",
    "np.random.shuffle(indices)\n",
    "print ('indices after shuffling',indices)\n",
    "# print (mydata[0])\n",
    "shuffled_data = [list(mydata[i]) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Separating features and labels from shuffled data\n",
    "\"\"\"\n",
    "shuffled_data = np.array(shuffled_data)\n",
    "X1 = shuffled_data[:,1:(num_features + 1)]\n",
    "Y1 = shuffled_data[:,:1]\n",
    "print (len(X1[0]))\n",
    "X = [X1[i]/LA.norm(X1[i]) for i in range(len(X1))]\n",
    "Y = [Y1[i][0] for i in range(len(Y1)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding 1 to each feature vector to incorporate offset parameter\n",
    "\"\"\"\n",
    "X = [ np.insert(X[i],0,1) for i in range(len(X))]\n",
    "d1 = len(X)\n",
    "print (len(X[0]))\n",
    "\n",
    "print (len(X))\n",
    "print (len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_blocks(X,Y,m):\n",
    "    \"\"\"\n",
    "    This function distributes features and corresponding lables to m nodes\n",
    "\n",
    "    Input\n",
    "    ---------\n",
    "     X: feature matrix, Y: labels, m: number of nodes\"\n",
    "\n",
    "    Returns\n",
    "     --------\n",
    "    features: A list containing features of all nodes\n",
    "    values: A list containing associated labels of all nodes\n",
    "\n",
    "    \"\"\"\n",
    "    BX = [[] for i in range(m)]\n",
    "    BY = [[] for i in range(m)]\n",
    "    d1 = len(X)\n",
    "    s2 = int(d1/m)  ## size of each block\n",
    "    g = 0\n",
    "    for i in range(m):\n",
    "        for j in range(g,s2 + g,1):\n",
    "            BX[i].append(X[j])\n",
    "            BY[i].append(Y[j])\n",
    "        g = g + s2 \n",
    "    samples_after_eq_dist = m*int(d1/m)  ## total samples after equally distributed samples to every node\n",
    "    remaining_samples = d1 - m*int(d1/m) ### This quantity will always be less than equal to m\n",
    "    if ( remaining_samples >= 1):\n",
    "        for j in range(remaining_samples):\n",
    "            BX[j].append(X[samples_after_eq_dist + j])\n",
    "            BY[j].append(Y[samples_after_eq_dist + j])\n",
    "\n",
    "    return [BX,BY]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 2D Torus topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_graph(n=0,create_using=None):\n",
    "    \"\"\"Return the empty graph with n nodes and zero edges.\n",
    "\n",
    "    Node labels are the integers 0 to n-1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if create_using is None:\n",
    "        # default empty graph is a simple graph\n",
    "        G=nx.Graph()\n",
    "    else:\n",
    "        G=create_using\n",
    "        G.clear()\n",
    "\n",
    "    G.add_nodes_from(range(n))\n",
    "    G.name=\"empty_graph(%d)\"%n\n",
    "    return G\n",
    "\n",
    "\n",
    "def grid_2d(m,n,periodic=False,create_using=None): ## m,n be the number of rows and number of \n",
    "    # columns in torus topolgy\n",
    "    \n",
    "    \"\"\" Return the 2d grid graph of mxn nodes,\n",
    "        each connected to its nearest neighbors.\n",
    "        Optional argument periodic=True will connect\n",
    "        boundary nodes via periodic boundary conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    G=empty_graph(0,create_using)\n",
    "    G.name=\"grid_2d_graph\"\n",
    "    rows=range(m)\n",
    "    columns=range(n)\n",
    "    G.add_nodes_from( (i,j) for i in rows for j in columns )\n",
    "    G.add_edges_from( ((i,j),(i-1,j)) for i in rows for j in columns if i>0 )\n",
    "    G.add_edges_from( ((i,j),(i,j-1)) for i in rows for j in columns if j>0 )\n",
    "    if G.is_directed():\n",
    "        G.add_edges_from( ((i,j),(i+1,j)) for i in rows for j in columns if i<m-1 )\n",
    "        G.add_edges_from( ((i,j),(i,j+1)) for i in rows for j in columns if j<n-1 )\n",
    "    if periodic:\n",
    "        if n>2:\n",
    "            G.add_edges_from( ((i,0),(i,n-1)) for i in rows )\n",
    "            if G.is_directed():\n",
    "                G.add_edges_from( ((i,n-1),(i,0)) for i in rows )\n",
    "        if m>2:\n",
    "            G.add_edges_from( ((0,j),(m-1,j)) for j in columns )\n",
    "            if G.is_directed():\n",
    "                G.add_edges_from( ((m-1,j),(0,j)) for j in columns )\n",
    "        G.name=\"periodic_grid_2d_graph(%d,%d)\"%(m,n)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing weight matrix W associated with 2D-Torus topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_graph(row,column,m,w1,w2):\n",
    "    \n",
    "    \"\"\"\n",
    "    row = column + 1\n",
    "    m: number of nodes\n",
    "    w1,w2: weights to generate entries of W \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    A = [[0 for j in range(m)] for i in range(m)] # weight matrix\n",
    "    W = [[0 for j in range(m)] for i in range(m)] # weight matrix\n",
    "    \n",
    "    \"Generating 2D Grid\"\n",
    "    G = grid_2d(row,column,periodic=False,create_using=None)\n",
    "    \n",
    "#     \"Adding extra edges to get 2D Torus\"\n",
    "#     edges_rows = [((0,0),(0,4)),((1,0),(1,4)),((2,0),(2,4)),((3,0),(3,4))] \n",
    "#     column_rows = [((0,0),(3,0)),((0,1),(3,1)),((0,2),(3,2)),((0,3),(3,3)),((0,4),(3,4))] \n",
    "#     G.add_edges_from(edges_rows)\n",
    "#     G.add_edges_from(column_rows)\n",
    "    nx.draw_networkx(G)\n",
    "    plt.savefig('2D-Grid')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \"Changing edges format to 1D so that it becomes easy to access the indices\"\n",
    "    \n",
    "    edges = []\n",
    "    for i in range(row):\n",
    "        for j in range(column-1):\n",
    "            edges.append([j+i*column,j+1+i*column])\n",
    "\n",
    "    for i in range(row-1):\n",
    "        for j in range(column):\n",
    "            edges.append([j+i*column,j+(i+1)*column])      \n",
    "\n",
    "    \"Adding extra edges to get 2D Torus from 2D grid\"\n",
    "\n",
    "    for i in range(column):\n",
    "        edges.append([i , i + (row-1)*column])\n",
    "\n",
    "    for i in range(row):\n",
    "        edges.append([i*column , row + i*column]) \n",
    "\n",
    "    print ('edges:',edges)   \n",
    "    print ('total edges in 2D Torus:',len(edges))\n",
    "    \n",
    "    for [u, v] in edges:\n",
    "        W[u][v] = rd.uniform(w1,w2)\n",
    "        W[v][u] = rd.uniform(w1,w2)\n",
    "\n",
    "    for i in range(m):\n",
    "#         W[i][i] = w1+3*w2\n",
    "        W[i][i] = m*w2\n",
    "       \n",
    "    for i in range(m):\n",
    "        s1 = sum(W[i])\n",
    "        W[i] = [W[i][j]/s1 for j in range(m)]       \n",
    "    for [u, v] in edges:\n",
    "        A[u][v] = min(W[u][v], W[v][u])\n",
    "        A[v][u] = min(W[u][v], W[v][u])\n",
    "    \n",
    "    for i in range(m):\n",
    "        A[i][i] = 1 - sum(A[i])\n",
    "\n",
    "    \n",
    "    B = np.matrix(A)\n",
    "    print ((B.transpose() == A).all()) ## check symmetric property of A. It will print True\n",
    "    print ([sum(A[i]) for i in range(m)])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "consensus algorithm, W is the weight matrix obtained from connected graph.\n",
    "This function returns epsilon approximation of the average of x_1, x_2, ...,x_m where x = [x_1,...,x_m]\n",
    "\"\"\"\n",
    "\n",
    "def acce_consensus(W,m,x,eta1,tau1):\n",
    "    \n",
    "    v = [[0 for i in range(len(x[0]))] for j in range(m)]\n",
    "\n",
    "    x_new1 = x\n",
    "    x_old1 = x\n",
    "\n",
    "    for t in range(int(tau1)):\n",
    "        x_old2 = list(x_old1)   ## z_k,t-1\n",
    "        x_old1 = list(x_new1)   ## z_k,t\n",
    "#         print ('z_k,t-1',x_old2[1][0])\n",
    "#         print ('z_k,t',x_old1[1][0])\n",
    "        for i in range(m):\n",
    "            v1 = [W[i][j]*np.array(x_old1[j]) for j in range(m)]\n",
    "            v[i] = [sum([v1[l][j] for l in range(m)]) for j in range(len(x[0]))] \n",
    "        first_term = (1+eta1)*np.array(v)\n",
    "        sec_term = eta1*np.array(x_old2)\n",
    "        x_new1 = np.subtract(first_term, sec_term)  ## z_{k,t+1}\n",
    "#         print ('z_k,t+1',x_new1[1][0])\n",
    "    return x_new1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function returns the weighted sum of x_1, x_2, ...,x_m where x = [x_1,...,x_m]\n",
    "\"\"\"\n",
    "\n",
    "def oneConsensus(W2,m,x):\n",
    "    v = [[0 for i in range(len(x[0]))] for j in range(m)]\n",
    "    for i in range(m):\n",
    "        u = [W2[i][j]*np.array(x[j]) for j in range(m)]\n",
    "        v[i] = [sum([u[q][j] for q in range(m)]) for j in range(len(x[i]))] \n",
    "        \n",
    "    return v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Momentum coefficient\n",
    "\"\"\"\n",
    "\n",
    "def update_t(k,alpha):\n",
    "    a1 = k\n",
    "    a2 = k + alpha \n",
    "    return a1/a2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient of logistic loss function at point x\n",
    "\"\"\"\n",
    "\n",
    "def logistic_grad(bFeature,bLabel,x,d1): ## d1 is the total number of samples\n",
    "    summ = np.zeros(len(bFeature[0]))\n",
    "#     cof = 2*lambdaa\n",
    "#     d1 = len(X)\n",
    "    a = len(bFeature) ## batch size\n",
    "    for i in range(a):\n",
    "        inp1 = np.dot(x,bFeature[i])\n",
    "        n1 = (-bLabel[i])*np.array(bFeature[i])\n",
    "        dom = bLabel[i]*(inp1)\n",
    "        if (dom > 0): ## To avoid math flow error\n",
    "            num1 = 1 - 1/(1 + math.exp((-1)*dom))\n",
    "            #print (num1)\n",
    "        else:\n",
    "            num1 = 1/(1 + math.exp(dom))\n",
    "        num2 = num1*np.array(n1) \n",
    "        summ = np.add(summ,num2)\n",
    "    loss_der = (1/d1)*np.array(summ)\n",
    "    return loss_der\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "logistic function value computed at point x\n",
    "\"\"\"\n",
    "\n",
    "def logistic_func(X,Y,x): \n",
    "    \"\"\"\n",
    "    logistic function value computed at x\n",
    "    Input:\n",
    "    Features: X\n",
    "    Labels: Y\n",
    "    regcoef: lambda\n",
    "    x: Point at which function value is computed\n",
    "\n",
    "    Returns:\n",
    "    Logistic regression function value at x with dataset (X,Y)\n",
    "    \"\"\"\n",
    "    f = 0\n",
    "    d1 = len(X)\n",
    "#     xnorm = (LA.norm(x))**2\n",
    "    for i in range(d1):\n",
    "        inp = np.dot(x,X[i])\n",
    "        dom = Y[i]*inp\n",
    "        if (dom > 0):\n",
    "            f1 = 1+ math.exp((-1)*dom)\n",
    "            z11 = math.log(f1)\n",
    "            f = f + z11   \n",
    "            #print (z11)\n",
    "        else:\n",
    "            f1 = 1+ math.exp(dom)\n",
    "            z11 = (-1)*dom + math.log(f1)\n",
    "            f = f + z11   \n",
    "            #print (z11)\n",
    "            \n",
    "         \n",
    "    func = f/(d1)\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Projection of point v onto l2 ball of radius R centred at v0, if v lies outside the ball \n",
    "then Proj(v) = v0 + (R/||v-v0||)(v-v0)\n",
    "\"\"\"\n",
    "\n",
    "def projection_l2ball_centre_v0(v,v0,R):\n",
    "    diff = np.subtract(v,v0)\n",
    "    norm = LA.norm(diff)\n",
    "#     print (norm)\n",
    "    if (norm <= R):\n",
    "#         print ('norm of projection point:',norm)\n",
    "        return v\n",
    "    else:\n",
    "        scaling = R/norm\n",
    "        scaled_vector = scaling*np.array(diff)\n",
    "        projection = np.add(v0,scaled_vector)\n",
    "#         print ('norm of projection point:',LA.norm(projection))\n",
    "\n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This function runs DAG (one step communication) starting from x0. It returns the average of local iterates and last iterate of all nodes. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def DAGOSC(x0,W,m,R,InIt,dataDis,step,epsilon_prevIte): ## d1 = total samples\n",
    "   \n",
    "    \"\"\"\n",
    "    x0: initial iterates x_{k,0}\n",
    "    W: weight matrix\n",
    "    m: number of nodes\n",
    "    InIt: number of inner iterates t_k\n",
    "    dataDis: local data sets\n",
    "    step: step size s_k\n",
    "    r: diameter of constraint set X\n",
    "    epsilon_prevIte: consensus error ub epsilon_k\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.zeros((nodes,len(X[0])))\n",
    "    func = np.zeros(InIt) # function values\n",
    "    x_wp = np.copy(x)\n",
    "    z1 = np.copy(x0)\n",
    "    y = np.copy(x0)\n",
    "    for t in range(InIt):\n",
    "        v3 = oneConsensus(W,m,y)\n",
    "        for i in range(m):\n",
    "            feature = dataDis[0][i] ## ith block of data\n",
    "            label = dataDis[1][i]\n",
    "            grad = logistic_grad(feature,label,y[i],d1)\n",
    "            step_grad = step*np.array(grad)\n",
    "            #norm_gradf[k] = norm_gradf[k] + (LA.norm(grad))**2    # norm of gradient\n",
    "            x_wp[i] = np.subtract(v3[i],step_grad)\n",
    "            radius_new = R + epsilon_prevIte ### R_k + epsilon(k)\n",
    "            x[i] = projection_l2ball_centre_v0(x_wp[i],x0[i],radius_new)\n",
    "            \n",
    "        if (t == 0):\n",
    "            y = x.copy()\n",
    "#             print ('y',y)\n",
    "        else:\n",
    "            mom1 = np.subtract(x,z1)\n",
    "#             print ('mom1',mom1[0])\n",
    "            beta = update_t(t,alpha)\n",
    "#             print ('beta',beta)\n",
    "            mom2 = beta*np.array(mom1)\n",
    "#             print ('mom2',mom2[0])\n",
    "            y = np.add(x,mom2)\n",
    "#             print ('y',y[0])\n",
    "        z1 = x.copy() ## store x^i of previous iterate\n",
    "        avg_estimates = np.mean(x,axis = 0)\n",
    "        func[t] = logistic_func(X,Y,avg_estimates)\n",
    "        function_mdaca.write(str(func[t])+'\\n')\n",
    "        function_mdaca.flush()\n",
    "    \n",
    "    return x  ## local iterates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" inner iterations \"\n",
    "def inner_ite(Rk,epsilonk,step_size,alpha,L,varepsilon0,k):\n",
    "    num1 = (alpha-1)*(Rk + epsilonk )*(2**(k/2))\n",
    "    den1 = math.sqrt(2*step_size)\n",
    "    num2 = 3*math.sqrt(2*L)*num1\n",
    "    den2 = math.sqrt(st ep_size*varepsilon0)\n",
    "    t1 = math.ceil(num1/den1)\n",
    "    t2 = math.ceil(num2/den2)\n",
    "    print ('first term in tk',t1)\n",
    "    print ('second term in tk',t2)\n",
    "    innite = max(t1,t2)\n",
    "#     print (type(innite))\n",
    "    print ('int(innite)',innite)\n",
    "    return innite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_esptilde_vareps(L,old_vareps,B,k):\n",
    "    new_eps = math.sqrt(old_vareps/(18*L**2))\n",
    "    new_epstilde = math.sqrt(old_vareps/18)\n",
    "    new_vareps = old_vareps/2\n",
    "    den = 1+L*(2**k -2)\n",
    "    new_omega = new_vareps + (6*L**2*B)/den + (new_vareps*L)/(3*den)\n",
    "\n",
    "    return new_eps,new_epstilde,new_vareps,new_omega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tau_iterates(nodes,Rk,old_eps,new_eps,muW):\n",
    "    \n",
    "    num1 = math.log(2*math.sqrt(nodes)*(Rk + 2*old_eps)) - math.log(new_eps)\n",
    "    den1 = (-1)*math.log(1-math.sqrt(1-muW))\n",
    "    tauk = math.ceil(num1/den1)\n",
    "    \n",
    "    return tauk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tau_gradients(L,x0,e0,nodes,old_eps,new_epstilde,muW):\n",
    "    \n",
    "    objx0 = logistic_func(X,Y,x0)\n",
    "    fterm = 4*nodes*L*(math.exp(1)*e0 + (math.exp(0.5)+1)*objx0 + (4/3)*math.exp(0.5) +2)\n",
    "    secterm = 4*nodes*L*objx0 + 2*nodes*(L**2)*(old_eps**2)\n",
    "    ## computing upper bound of ||grad F(x_{k+1,0})||\n",
    "    gradub = math.sqrt(fterm + secterm)\n",
    "    \n",
    "    num1 = math.log(2*gradub +2) - math.log(new_epstilde)\n",
    "    den1 = (-1)*math.log(1-math.sqrt(1-muW))\n",
    "    \n",
    "    tautilde = math.ceil(num1/den1)\n",
    "    return tautilde\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepsize(L,nodes,B,vareps0,lambdaa_minW,muW,k):\n",
    "    \n",
    "    fterm = lambdaa_minW/L\n",
    "#     maxfterm = (72*nodes*B*(L**2))/vareps0\n",
    "\n",
    "#     den1 = 2*(max(maxfterm,nodes*L)*(2**k) + 2*nodes*L + L)\n",
    "    den1 = 0.0001+L*(2**k)\n",
    "    secterm = (1-muW)/den1\n",
    "    print ('fterm, secterm:',fterm,secterm)\n",
    "    if (fterm >= secterm):\n",
    "        return fterm/(2**k)\n",
    "    else:\n",
    "        return secterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mDACA(X,Y,T,nodes,W,d1,alpha,zeta,eta1,c_0):\n",
    "\n",
    "#     z = [[0 for i in range(len(X[0]))]  for j in range(m)] \n",
    "    z = np.zeros((nodes,len(X[0])))\n",
    "    initialx0 = np.zeros(len(X[0]))\n",
    "#     func = np.zeros(T+1) # function values\n",
    "    dataDis = data_blocks(X,Y,nodes)\n",
    "    func = logistic_func(X,Y,z[0])\n",
    "    function_mdaca.write(str(func)+'\\n')\n",
    "    function_mdaca.flush()\n",
    "    tauIt = np.zeros(T) ## stores number of communications in consensus on iterates\n",
    "    tauGrad = np.zeros(T) ## stores number of communications in consensus on gradients\n",
    "    c_val = np.zeros(T) ## stores c_k at every outer iterate\n",
    "    innIter = np.zeros(T)\n",
    "    cycles = np.ones(T) ## stores number of cycles at every outer iterate k\n",
    "    xCons = np.copy(z)\n",
    "    xCons_old = np.copy(z) ## consensus iterates at previous outer iterate\n",
    "    \"Initialize varpsilon_0\"\n",
    "    grad_0 = logistic_grad(X,Y,xCons[0],d1)\n",
    "    vareps0 = 4 + 4*(LA.norm(grad_0))**2 + 10**7\n",
    "    print ('varepsilon0',vareps0)\n",
    "    new_eps = 1 ## epsilon_1\n",
    "    new_epstilde = 1 ## epsilontilde_1\n",
    "    new_vareps = vareps0/2 ## varepsilon_1 \n",
    "    new_omega = vareps0/2\n",
    "    c = c_0\n",
    "    B = func ## f(x0) because f^lb = 0 for logistic loss\n",
    "    e0 = 2*B ## choosing e0 = 2f(x0)\n",
    "    for k in range(T):\n",
    "        \"\"\"\n",
    "        Outer iteration\n",
    "        \"\"\"\n",
    "        step = stepsize(L,nodes,B,vareps0,lambdaa_minW,muW,k)\n",
    "#         if (k <= 20): ## using large step size in the beginning\n",
    "#         step = lambdaa_minW/(L*2**k)\n",
    "        print ('step size at k=',k+1,'is',step)\n",
    "        ## store old epsilons before updating them\n",
    "        old_eps = np.copy(new_eps)\n",
    "        old_epstilde = np.copy(new_epstilde)\n",
    "        old_vareps = np.copy(new_vareps)\n",
    "        old_omega = np.copy(new_omega)\n",
    "        \n",
    "        ## compute new epsilon, epsilon tilde, varepsilon\n",
    "        new_eps,new_epstilde,new_vareps,new_omega = eps_esptilde_vareps(L,old_vareps,B,k+1)\n",
    "        ## compute radius Rk\n",
    "        Rk = (c**2)*(math.sqrt(old_omega)+old_epstilde+L*old_eps)\n",
    "        print ('radius Rk at k=',k+1,'is',Rk)\n",
    "#         print (tauIt[k])\n",
    "#         print (tauGrad[k])\n",
    "        ## compute inner iterations tk\n",
    "        inIt = inner_ite(Rk,old_eps,step,alpha,L,vareps0,k+1)\n",
    "        innIter[k] = inIt\n",
    "        print ('inner iterations at k =',k+1,'are',innIter[k])\n",
    "\n",
    "#         InIt = 10**4\n",
    "        inner_iterates_cycles.write(str(innIter[k])+'\\n')\n",
    "        inner_iterates_cycles.flush()\n",
    "#         if (k <= 10): ## using large step size in the beginning\n",
    "#             step = lambdaa_minW/L\n",
    "#         print ('step size:',step)\n",
    "#         np.savetxt('innIter_outItek_check.txt',innIter) ## store inner iterations for every outer iteration k\n",
    "        z = DAGOSC(xCons_old,W,nodes,Rk,inIt,dataDis,step,old_eps)\n",
    "    \n",
    "        ## compute tau_k+1 and tau_tilde_k+1\n",
    "        tauIt[k] = tau_iterates(nodes,Rk,old_eps,new_eps,muW)\n",
    "        num_comm_iterates.write(str(tauIt[k])+'\\n')\n",
    "        num_comm_iterates.flush()\n",
    "        xCons = acce_consensus(W,nodes,z,eta1,tauIt[k])\n",
    "        \n",
    "        grad_est = [] ## stores grad f_i at consensus iterate xCons^i\n",
    "        for i in range(nodes):\n",
    "            feature = dataDis[0][i] ## ith block of data\n",
    "            label = dataDis[1][i]\n",
    "            grad_est.append(logistic_grad(feature,label,xCons[i],d1))\n",
    "        \n",
    "        tauGrad[k] = tau_gradients(L,initialx0,e0,nodes,old_eps,new_epstilde,muW)  ## tilde{tau}_k\n",
    "        grad_cons = acce_consensus(W,nodes,grad_est,eta1,tauGrad[k])  ## cal F_{k,0}\n",
    "        num_comm_gradients.write(str(tauGrad[k])+'\\n')\n",
    "        num_comm_gradients.flush()\n",
    "        \n",
    "#         varepsln = varepsilon(g1,g2,g3,varepsilon_prevIte,c,step,k)\n",
    "        print ('varepsilon at previous iterate',old_vareps)\n",
    "#         print ('varepsilon at k =',k+1,'is',new_vareps)\n",
    "        normGradC = [(LA.norm(grad_cons[i]))**2 for i in range(nodes)]\n",
    "#         print ('norm of cal Fi_s at k = ',k+1,'are',normGradC)\n",
    "        check = [normGradC[i] <= new_vareps for i in range(nodes) ]\n",
    "        print ('gradient norm at all nodes (cal F)',normGradC)\n",
    "        print ('varepsilon(k+1)',new_vareps)\n",
    "        while (any(check) == False): ## any(check) returns True if atleast one is True in check array otherwie returns False\n",
    "            cycles[k] += 1\n",
    "            c = zeta*c\n",
    "            Rk = (c**2)*(math.sqrt(old_omega)+old_epstilde+L*old_eps)\n",
    "    #         print (tauIt[k])\n",
    "    #         print (tauGrad[k])\n",
    "            ## compute inner iterations tk\n",
    "            inIt = inner_ite(Rk,old_eps,step,alpha,L,vareps0,k+1)\n",
    "            innIter[k] = inIt\n",
    "            print ('inner iterations at k =',k+1,'are',innIter[k])\n",
    "\n",
    "    #         InIt = 10**4\n",
    "            inner_iterates_cycles.write(str(innIter[k])+'\\n')\n",
    "            inner_iterates_cycles.flush()\n",
    "\n",
    "    #         np.savetxt('innIter_outItek_check.txt',innIter) ## store inner iterations for every outer iteration k\n",
    "            z = DAGOSC(xCons_old,W,nodes,Rk,inIt,dataDis,step,old_eps)\n",
    "\n",
    "            ## compute tau_k+1 and tau_tilde_k+1\n",
    "            tauIt[k] = tau_iterates(nodes,Rk,old_eps,new_eps,muW)\n",
    "            num_comm_iterates.write(str(tauIt[k])+'\\n')\n",
    "            num_comm_iterates.flush()\n",
    "            xCons = acce_consensus(W,nodes,z,eta1,tauIt[k])\n",
    "\n",
    "            grad_est = [] ## stores grad f_i at consensus iterate xCons^i\n",
    "            for i in range(nodes):\n",
    "                feature = dataDis[0][i] ## ith block of data\n",
    "                label = dataDis[1][i]\n",
    "                grad_est.append(logistic_grad(feature,label,xCons[i],d1))\n",
    "\n",
    "            tauGrad[k] = tau_gradients(L,initialx0,e0,nodes,old_eps,new_epstilde,muW)  ## tilde{tau}_k\n",
    "            grad_cons = acce_consensus(W,nodes,grad_est,eta1,tauGrad[k])  ## cal F_{k,0}\n",
    "            num_comm_gradients.write(str(tauGrad[k])+'\\n')\n",
    "            num_comm_gradients.flush()\n",
    "\n",
    "    #         varepsln = varepsilon(g1,g2,g3,varepsilon_prevIte,c,step,k)\n",
    "            print ('varepsilon at previous iterate',old_vareps)\n",
    "            print ('varepsilon at k =',k+1,'is',new_vareps)\n",
    "            normGradC = [(LA.norm(grad_cons[i]))**2 for i in range(nodes)]\n",
    "    #         print ('norm of cal Fi_s at k = ',k+1,'are',normGradC)\n",
    "            check = [normGradC[i] <= new_vareps for i in range(nodes) ]\n",
    "        \n",
    "        \n",
    "        xCons_old = np.copy(xCons)  \n",
    "        c_val[k] = c\n",
    "        c_values_OuterIte.write(str(c_val[k])+'\\n')\n",
    "        c_values_OuterIte.flush()\n",
    "        print ('outer iteration' ,k+1,'done')\n",
    "                            \n",
    "                 \n",
    "    return True\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing Lipschitz parameter\n",
    "\n",
    "X3 = np.matrix(X)\n",
    "\n",
    "M1 = np.matmul(np.transpose(X3),X3)\n",
    "M = M1/len(X3)\n",
    "from numpy.linalg import eig\n",
    "values , vectors = eig(M)\n",
    "print (max(values)) ## L is less than equal to maximum eigen value\n",
    "\n",
    "L = float(max(values))\n",
    "print (L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"creating files to store output\"\n",
    "\n",
    "function_mdaca = open(r\"func_mDACA_a4a_2dTorus.txt\",\"w\")\n",
    "inner_iterates_cycles = open(r\"inner_iterates_involving_in_cycles.txt\",\"w\")\n",
    "num_comm_iterates = open(r\"num_comm_iterates.txt\",\"w\")\n",
    "num_comm_gradients = open(r\"num_comm_gradients.txt\",\"w\")\n",
    "c_values_OuterIte = open(r\"c_values.txt\",\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"Initializations\"\n",
    "\n",
    "row = 4 ### total number of rows in 2D Torus\n",
    "column = 5 ### column = row+1, total number of columns in 2D Torus\n",
    "nodes = row*column ### total number of nodes\n",
    "\n",
    "d1 = len(X) ## total number of samples \n",
    "T = 500 ## number of outer iteartions\n",
    "alpha = 10\n",
    "c_0 = 2\n",
    "zeta = math.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Generate weight matrix W and compute its eigenvalues\"\n",
    "\n",
    "rd.seed(100)\n",
    "W = gen_graph(row,column,nodes,0.09,0.2)\n",
    "\n",
    "np.savetxt('weight_mat.txt',W)\n",
    "from numpy.linalg import eig\n",
    "values , vectors = eig(W)\n",
    "values = np.sort(values)\n",
    "print('eigen values',values)\n",
    "muW = values[nodes-2] ## second largest eigenvalue of W\n",
    "print ('second largest eigenvalue of W:',muW)\n",
    "lambdaa_minW = values[0] ## smallest eigenvalue of W\n",
    "print ('smallest eigenvalue of W:',lambdaa_minW)\n",
    "\n",
    "eta1 = (1 - math.sqrt(1 - muW**2))/(1 + math.sqrt(1 - muW**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data partition\n",
    "\n",
    "[BX,BY] = data_blocks(X,Y,nodes)\n",
    "for i in range(nodes):\n",
    "    minus1 = BY[i].count(-1)\n",
    "    one  =  BY[i].count(1)\n",
    "\n",
    "minus1 = Y.count(-1)\n",
    "one = Y.count(1)\n",
    "\n",
    "print (len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "func = mDACA(X,Y,T,nodes,W,d1,alpha,zeta,eta1,c_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
